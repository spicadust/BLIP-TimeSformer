{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reijimc-ubuntu/Repos/BLIP-TimeSformer/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import cell\n",
    "'''\n",
    " * Copyright (c) 2022, salesforce.com, inc.\n",
    " * All rights reserved.\n",
    " * SPDX-License-Identifier: BSD-3-Clause\n",
    " * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n",
    " * By Junnan Li\n",
    "'''\n",
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.blip_retrieval import blip_retrieval\n",
    "import utils\n",
    "from data.video_dataset import VideoDataset\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "GPU Memory before clearing: 0.00MB\n",
      "GPU Memory after clearing: 0.00MB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    # Print memory stats before and after clearing\n",
    "    print(f\"GPU Memory before clearing: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"GPU Memory after clearing: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluation(model, data_loader, tokenizer, device, config):\n",
    "    # test\n",
    "    model.eval() \n",
    "    \n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Evaluation:'    \n",
    "    \n",
    "    # Add these three lines at the start\n",
    "    num_text = len(data_loader.dataset.text)\n",
    "    num_videos = len(data_loader)\n",
    "    print(f'Starting evaluation with {num_text} texts and {num_videos} videos...')\n",
    "          \n",
    "    print('Computing features for evaluation...')\n",
    "    start_time = time.time()  \n",
    "\n",
    "    texts = data_loader.dataset.text   \n",
    "    num_text = len(texts)\n",
    "    text_bs = 256\n",
    "    text_ids = []\n",
    "    text_embeds = []  \n",
    "    text_atts = []\n",
    "\n",
    "    # Replace this for-loop\n",
    "    print('Computing text features...')\n",
    "    for i in tqdm(range(0, num_text, text_bs), desc=\"Processing text batches\"):\n",
    "        text = texts[i: min(num_text, i+text_bs)]\n",
    "        text_input = tokenizer(text, padding='max_length', truncation=True, max_length=35, return_tensors=\"pt\").to(device) \n",
    "        text_output = model.text_encoder(text_input.input_ids, attention_mask = text_input.attention_mask, mode='text')  \n",
    "        text_embed = F.normalize(model.text_proj(text_output.last_hidden_state[:,0,:]))\n",
    "        text_embeds.append(text_embed)   \n",
    "        text_ids.append(text_input.input_ids)\n",
    "        text_atts.append(text_input.attention_mask)\n",
    "    \n",
    "    text_embeds = torch.cat(text_embeds,dim=0)\n",
    "    text_ids = torch.cat(text_ids,dim=0)\n",
    "    text_atts = torch.cat(text_atts,dim=0)\n",
    "    text_ids[:,0] = tokenizer.additional_special_tokens_ids[0]\n",
    "    \n",
    "    \n",
    "    print('Computing video features...')\n",
    "    video_feats = []\n",
    "    video_embeds = []\n",
    "    # Replace this for-loop\n",
    "    for video, video_id in tqdm(data_loader, desc=\"Processing videos\"): \n",
    "        B,N,C,W,H = video.size()\n",
    "        video = video.view(-1,C,W,H)\n",
    "        video = video.to(device,non_blocking=True) \n",
    "        video_feat = model.visual_encoder(video)        \n",
    "        video_embed = model.vision_proj(video_feat[:,0,:])   \n",
    "        video_embed = video_embed.view(B,N,-1).mean(dim=1)\n",
    "        video_embed = F.normalize(video_embed,dim=-1)  \n",
    "       \n",
    "        video_feat = video_feat.view(B,-1,video_feat.shape[-1])\n",
    "        video_feats.append(video_feat.cpu())\n",
    "        video_embeds.append(video_embed)\n",
    "     \n",
    "    video_feats = torch.cat(video_feats,dim=0)\n",
    "    video_embeds = torch.cat(video_embeds,dim=0)\n",
    "    \n",
    "    sims_matrix = video_embeds @ text_embeds.t()\n",
    "    score_matrix_v2t = torch.full((len(texts),len(texts)),-100.0).to(device) \n",
    "    \n",
    "    num_tasks = utils.get_world_size()\n",
    "    rank = utils.get_rank() \n",
    "    step = sims_matrix.size(0)//num_tasks + 1\n",
    "    start = rank*step\n",
    "    end = min(sims_matrix.size(0),start+step)\n",
    "\n",
    "    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)): \n",
    "        topk_sim, topk_idx = sims.topk(k=config['k_test'], dim=0)\n",
    "        \n",
    "        encoder_output = video_feats[start+i].repeat(config['k_test'],1,1).to(device,non_blocking=True) \n",
    "        encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device,non_blocking=True) \n",
    "        output = model.text_encoder(text_ids[topk_idx], \n",
    "                                    attention_mask = text_atts[topk_idx],\n",
    "                                    encoder_hidden_states = encoder_output,\n",
    "                                    encoder_attention_mask = encoder_att,                             \n",
    "                                    return_dict = True,\n",
    "                                   )\n",
    "        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\n",
    "        score_matrix_v2t[start+i,topk_idx] = score + topk_sim\n",
    "        \n",
    "    sims_matrix = sims_matrix.t()\n",
    "    score_matrix_t2v = torch.full((len(texts),len(texts)),-100.0).to(device) \n",
    "    \n",
    "    step = sims_matrix.size(0)//num_tasks + 1\n",
    "    start = rank*step\n",
    "    end = min(sims_matrix.size(0),start+step)    \n",
    "    \n",
    "    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)): \n",
    "        \n",
    "        topk_sim, topk_idx = sims.topk(k=config['k_test'], dim=0)\n",
    "        topk_idx = topk_idx.cpu()  # Move indices to CPU\n",
    "        encoder_output = video_feats[topk_idx].to(device,non_blocking=True) \n",
    "        encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device,non_blocking=True) \n",
    "        output = model.text_encoder(text_ids[start+i].repeat(config['k_test'],1), \n",
    "                                    attention_mask = text_atts[start+i].repeat(config['k_test'],1),\n",
    "                                    encoder_hidden_states = encoder_output,\n",
    "                                    encoder_attention_mask = encoder_att,                             \n",
    "                                    return_dict = True,\n",
    "                                   )\n",
    "        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\n",
    "        score_matrix_t2v[start+i,topk_idx] = score + topk_sim\n",
    "\n",
    "    if args.distributed:\n",
    "        dist.barrier()   \n",
    "        torch.distributed.all_reduce(score_matrix_v2t, op=torch.distributed.ReduceOp.SUM) \n",
    "        torch.distributed.all_reduce(score_matrix_t2v, op=torch.distributed.ReduceOp.SUM)        \n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Evaluation time {}'.format(total_time_str)) \n",
    "\n",
    "    return score_matrix_v2t.cpu().numpy(), score_matrix_t2v.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Metric calculation function            \n",
    "@torch.no_grad()\n",
    "def itm_eval(scores_v2t, scores_t2v, txt2vmg, vid2txt):\n",
    "    \n",
    "    #Video->Text \n",
    "    ranks = np.zeros(scores_v2t.shape[0])\n",
    "    for index,score in enumerate(scores_v2t):\n",
    "        inds = np.argsort(score)[::-1]\n",
    "        ranks[index] = np.where(inds == vid2txt[index])[0][0]\n",
    "\n",
    "    # Compute metrics\n",
    "    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
    "  \n",
    "    #Text->Video \n",
    "    ranks = np.zeros(scores_t2v.shape[0])\n",
    "    \n",
    "    for index,score in enumerate(scores_t2v):\n",
    "        inds = np.argsort(score)[::-1]\n",
    "        ranks[index] = np.where(inds == txt2vmg[index])[0][0]\n",
    "    \n",
    "    mdR = np.median(ranks+1)\n",
    "        \n",
    "    # Compute metrics\n",
    "    vr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "    vr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "    vr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)        \n",
    "\n",
    "    tr_mean = (tr1 + tr5 + tr10) / 3\n",
    "    vr_mean = (vr1 + vr5 + vr10) / 3\n",
    "    r_mean = (tr_mean + vr_mean) / 2\n",
    "\n",
    "    eval_result =  {'txt_r1': tr1,\n",
    "                    'txt_r5': tr5,\n",
    "                    'txt_r10': tr10,\n",
    "                    'txt_r_mean': tr_mean,\n",
    "                    'vid_r1': vr1,\n",
    "                    'vid_r5': vr5,\n",
    "                    'vid_r10': vr10,\n",
    "                    'vid_r_mean': vr_mean,\n",
    "                    'vid_mdR': mdR,\n",
    "                    'r_mean': r_mean}\n",
    "    return eval_result\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "def main(args, config):\n",
    "    utils.init_distributed_mode(args)    \n",
    "    \n",
    "    device = torch.device(args.device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    #### Dataset #### \n",
    "    print(\"Creating retrieval dataset\")\n",
    "    test_dataset = VideoDataset(config['video_root'],config['ann_root'],num_frm=config['num_frm_test'],\n",
    "                                max_img_size=config['image_size'], frm_sampling_strategy='uniform') \n",
    "    \n",
    "    # Add these diagnostic prints\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Video directory: {config['video_root']}\")\n",
    "    print(f\"Number of videos found: {len(test_dataset)}\")\n",
    "    print(f\"Number of videos in annotation: {len(test_dataset.video2txt)}\")\n",
    "    \n",
    "    # List a few video paths to verify\n",
    "    video_files = os.listdir(config['video_root'])\n",
    "    mp4_files = [f for f in video_files if f.endswith('.mp4')]\n",
    "    print(f\"\\nTotal MP4 files in directory: {len(mp4_files)}\")\n",
    "    print(\"First few videos in directory:\", mp4_files[:5])\n",
    "\n",
    "    print(f\"Batch size: {config['batch_size']}\")\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "    )  \n",
    "\n",
    "    #### Model #### \n",
    "    print(\"Creating model\")\n",
    "    model = blip_retrieval(pretrained=config['pretrained'], image_size=config['image_size'], vit=config['vit'])\n",
    "    \n",
    "    model = model.to(device)   \n",
    "    \n",
    "    model_without_ddp = model\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        model_without_ddp = model.module   \n",
    "    \n",
    "    score_v2t, score_t2v, = evaluation(model_without_ddp, test_loader, model_without_ddp.tokenizer, device, config)\n",
    "\n",
    "    if utils.is_main_process():  \n",
    "\n",
    "        test_result = itm_eval(score_v2t, score_t2v, test_loader.dataset.txt2video, test_loader.dataset.video2txt)  \n",
    "        print(test_result)\n",
    "\n",
    "        log_stats = {**{f'{k}': v for k, v in test_result.items()},}\n",
    "        with open(os.path.join(args.output_dir, \"test_result.txt\"),\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")     \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration cell - replace argparse\n",
    "config_path = './configs/retrieval_msrvtt.yaml'\n",
    "yaml = ruamel.yaml.YAML(typ='safe')  # Create a YAML object with safe loading\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "# Define args as a simple namespace object\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.config = config_path\n",
    "        self.output_dir = 'output/Retrieval_msrvtt'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.seed = 42\n",
    "        self.world_size = 1\n",
    "        self.dist_url = 'env://'\n",
    "        self.distributed = False  # Changed to False for notebook usage\n",
    "        self.gpu = 0\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Create output directory\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config updated:\n",
      "- Changed ann_root from 'annotation/data/small_msrvtt_ret' to 'annotation/data/small_msrvtt_ret'\n",
      "\n",
      "Verification:\n",
      "- Annotation directory exists: True\n",
      "- Video directory exists: True\n",
      "- Found annotation files: ['val.jsonl', 'test.jsonl', 'train.jsonl']\n"
     ]
    }
   ],
   "source": [
    "def update_config():\n",
    "    # Load the current config\n",
    "    yaml = ruamel.yaml.YAML()\n",
    "    config_path = './configs/retrieval_msrvtt.yaml'\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.load(f)\n",
    "    \n",
    "    # Update the annotation path\n",
    "    old_path = config['ann_root']\n",
    "    config['ann_root'] = 'annotation/data/small_msrvtt_ret'\n",
    "    \n",
    "    # Save the updated config\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    print(\"Config updated:\")\n",
    "    print(f\"- Changed ann_root from '{old_path}' to '{config['ann_root']}'\")\n",
    "    print(\"\\nVerification:\")\n",
    "    print(f\"- Annotation directory exists: {os.path.exists(config['ann_root'])}\")\n",
    "    print(f\"- Video directory exists: {os.path.exists(config['video_root'])}\")\n",
    "    print(f\"- Found annotation files: {os.listdir(os.path.join(config['ann_root'], 'txt'))}\")\n",
    "\n",
    "update_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch calculations for each split:\n",
      "Batch size: 64\n",
      "--------------------------------------------------\n",
      "\n",
      "train.jsonl:\n",
      "- Unique videos: 7010\n",
      "- Number of batches: 110\n",
      "- Total captions: 140200\n",
      "\n",
      "val.jsonl:\n",
      "- Unique videos: 1000\n",
      "- Number of batches: 16\n",
      "- Total captions: 1000\n",
      "\n",
      "test.jsonl:\n",
      "- Unique videos: 1000\n",
      "- Number of batches: 16\n",
      "- Total captions: 1000\n",
      "\n",
      "Total across all splits:\n",
      "- Total unique videos: 9010\n",
      "- Total batches if processing all splits: 141\n",
      "\n",
      "Estimated memory usage per batch:\n",
      "- Batch size: 64 videos\n",
      "- Approximate memory per batch: 3200MB\n"
     ]
    }
   ],
   "source": [
    "def calculate_batch_info():\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    batch_size = 64  # from your config\n",
    "    txt_dir = Path('./annotation/data/msrvtt_ret/txt')\n",
    "    \n",
    "    print(\"Batch calculations for each split:\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total_videos = 0\n",
    "    for split in ['train.jsonl', 'val.jsonl', 'test.jsonl']:\n",
    "        file_path = txt_dir / split\n",
    "        if file_path.exists():\n",
    "            with open(file_path) as f:\n",
    "                lines = f.readlines()\n",
    "                unique_videos = len(set(json.loads(line)['clip_name'] for line in lines))\n",
    "                num_batches = (unique_videos + batch_size - 1) // batch_size\n",
    "                total_videos += unique_videos\n",
    "                \n",
    "                print(f\"\\n{split}:\")\n",
    "                print(f\"- Unique videos: {unique_videos}\")\n",
    "                print(f\"- Number of batches: {num_batches}\")\n",
    "                print(f\"- Total captions: {len(lines)}\")\n",
    "    \n",
    "    print(f\"\\nTotal across all splits:\")\n",
    "    print(f\"- Total unique videos: {total_videos}\")\n",
    "    print(f\"- Total batches if processing all splits: {(total_videos + batch_size - 1) // batch_size}\")\n",
    "    \n",
    "    # Memory usage estimate (rough calculation)\n",
    "    avg_video_size_mb = 50  # rough estimate, adjust based on your actual videos\n",
    "    print(f\"\\nEstimated memory usage per batch:\")\n",
    "    print(f\"- Batch size: {batch_size} videos\")\n",
    "    print(f\"- Approximate memory per batch: {batch_size * avg_video_size_mb}MB\")\n",
    "\n",
    "calculate_batch_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Using device: cuda\n",
      "Creating retrieval dataset\n",
      "Using downloaded and verified file: annotation/data/small_msrvtt_ret/msrvtt_test.jsonl\n",
      "\n",
      "Dataset Statistics:\n",
      "Video directory: ./small_video_data\n",
      "Number of videos found: 100\n",
      "Number of videos in annotation: 100\n",
      "\n",
      "Total MP4 files in directory: 100\n",
      "First few videos in directory: ['video7754.mp4', 'video9548.mp4', 'video9205.mp4', 'video9200.mp4', 'video9207.mp4']\n",
      "Batch size: 32\n",
      "Creating model\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n",
      "missing keys:\n",
      "[]\n",
      "Starting evaluation with 100 texts and 4 videos...\n",
      "Computing features for evaluation...\n",
      "Computing text features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text batches: 100%|██████████| 1/1 [00:00<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing video features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 4/4 [00:17<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:  [  0/100]  eta: 0:00:29    time: 0.2996  data: 0.0001  max mem: 12674\n",
      "Evaluation:  [ 50/100]  eta: 0:00:13    time: 0.2718  data: 0.0000  max mem: 12674\n",
      "Evaluation:  [ 99/100]  eta: 0:00:00    time: 0.2723  data: 0.0000  max mem: 12674\n",
      "Evaluation: Total time: 0:00:26 (0.2700 s / it)\n",
      "Evaluation:  [  0/100]  eta: 0:00:29    time: 0.2956  data: 0.0001  max mem: 12674\n",
      "Evaluation:  [ 50/100]  eta: 0:00:14    time: 0.2885  data: 0.0000  max mem: 12674\n",
      "Evaluation:  [ 99/100]  eta: 0:00:00    time: 0.2889  data: 0.0000  max mem: 12674\n",
      "Evaluation: Total time: 0:00:28 (0.2887 s / it)\n",
      "Evaluation time 0:01:14\n",
      "{'txt_r1': 59.0, 'txt_r5': 87.0, 'txt_r10': 94.0, 'txt_r_mean': 80.0, 'vid_r1': 63.0, 'vid_r5': 86.0, 'vid_r10': 90.0, 'vid_r_mean': 79.66666666666667, 'vid_mdR': np.float64(1.0), 'r_mean': 79.83333333333334}\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    main(args, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
