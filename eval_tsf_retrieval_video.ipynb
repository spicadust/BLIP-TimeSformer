{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reijimc-ubuntu/Repos/BLIP-TimeSformer/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import cell\n",
    "'''\n",
    " * Copyright (c) 2022, salesforce.com, inc.\n",
    " * All rights reserved.\n",
    " * SPDX-License-Identifier: BSD-3-Clause\n",
    " * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n",
    " * By Junnan Li\n",
    "'''\n",
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.blip_tsf_retrieval import blip_tsf_retrieval\n",
    "import utils\n",
    "from data.video_dataset import VideoDataset\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "GPU Memory before clearing: 0.00MB\n",
      "GPU Memory after clearing: 0.00MB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    # Print memory stats before and after clearing\n",
    "    print(f\"GPU Memory before clearing: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"GPU Memory after clearing: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluation(model, data_loader, tokenizer, device, config):\n",
    "    # test\n",
    "    model.eval() \n",
    "    \n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Evaluation:'    \n",
    "    \n",
    "    # Add these three lines at the start\n",
    "    num_text = len(data_loader.dataset.text)\n",
    "    num_videos = len(data_loader)\n",
    "    print(f'Starting evaluation with {num_text} texts and {num_videos} videos...')\n",
    "          \n",
    "    print('Computing features for evaluation...')\n",
    "    start_time = time.time()  \n",
    "\n",
    "    texts = data_loader.dataset.text\n",
    "    num_text = len(texts)\n",
    "    text_bs = 256\n",
    "    text_ids = []\n",
    "    text_embeds = []  \n",
    "    text_atts = []\n",
    "\n",
    "    # Replace this for-loop\n",
    "    print('Computing text features...')\n",
    "    for i in tqdm(range(0, num_text, text_bs), desc=\"Processing text batches\"):\n",
    "        text = texts[i: min(num_text, i+text_bs)]\n",
    "        text_input = tokenizer(text, padding='max_length', truncation=True, max_length=35, return_tensors=\"pt\").to(device) \n",
    "        text_output = model.text_encoder(text_input.input_ids, attention_mask = text_input.attention_mask, mode='text')  \n",
    "        text_embed = F.normalize(model.text_proj(text_output.last_hidden_state[:,0,:]))\n",
    "        text_embeds.append(text_embed)   \n",
    "        text_ids.append(text_input.input_ids)\n",
    "        text_atts.append(text_input.attention_mask)\n",
    "    \n",
    "    text_embeds = torch.cat(text_embeds,dim=0)\n",
    "    text_ids = torch.cat(text_ids,dim=0)\n",
    "    text_atts = torch.cat(text_atts,dim=0)\n",
    "    text_ids[:,0] = tokenizer.additional_special_tokens_ids[0]\n",
    "    \n",
    "    \n",
    "    print('Computing video features...')\n",
    "    video_feats = []\n",
    "    video_embeds = []\n",
    "    # Replace this for-loop\n",
    "    for video, video_id in tqdm(data_loader, desc=\"Processing videos\"): \n",
    "        B,N,C,W,H = video.size()\n",
    "        # video = video.view(-1,C,W,H)\n",
    "        \n",
    "        # Permute dimensions to: [B, C, N, H, W]\n",
    "        video = video.permute(0, 2, 1, 4, 3)\n",
    "\n",
    "        video = video.to(device,non_blocking=True)\n",
    "        video_feat = model.visual_encoder(video)\n",
    "        video_embed = model.vision_proj(video_feat[:,0,:])   \n",
    "        video_embed = video_embed.view(B,N,-1).mean(dim=1)\n",
    "        video_embed = F.normalize(video_embed,dim=-1)  \n",
    "       \n",
    "        video_feat = video_feat.view(B,-1,video_feat.shape[-1])\n",
    "        video_feats.append(video_feat.cpu())\n",
    "        video_embeds.append(video_embed)\n",
    "     \n",
    "    video_feats = torch.cat(video_feats,dim=0)\n",
    "    video_embeds = torch.cat(video_embeds,dim=0)\n",
    "    \n",
    "    print(f\"video_feats shape: {video_feats.shape}\")\n",
    "    print(f\"video_embeds shape: {video_embeds.shape}\")\n",
    "    print(f\"text_embeds shape: {text_embeds.shape}\")\n",
    "    \n",
    "    sims_matrix = video_embeds @ text_embeds.t()\n",
    "    score_matrix_v2t = torch.full((len(texts),len(texts)),-100.0).to(device) \n",
    "    \n",
    "    num_tasks = utils.get_world_size()\n",
    "    rank = utils.get_rank() \n",
    "    step = sims_matrix.size(0)//num_tasks + 1\n",
    "    start = rank*step\n",
    "    end = min(sims_matrix.size(0),start+step)\n",
    "\n",
    "    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)): \n",
    "        topk_sim, topk_idx = sims.topk(k=config['k_test'], dim=0)\n",
    "        \n",
    "        encoder_output = video_feats[start+i].repeat(config['k_test'],1,1).to(device,non_blocking=True) \n",
    "        encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device,non_blocking=True) \n",
    "        output = model.text_encoder(text_ids[topk_idx], \n",
    "                                    attention_mask = text_atts[topk_idx],\n",
    "                                    encoder_hidden_states = encoder_output,\n",
    "                                    encoder_attention_mask = encoder_att,                             \n",
    "                                    return_dict = True,\n",
    "                                   )\n",
    "        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\n",
    "        score_matrix_v2t[start+i,topk_idx] = score + topk_sim\n",
    "        \n",
    "    sims_matrix = sims_matrix.t()\n",
    "    score_matrix_t2v = torch.full((len(texts),len(texts)),-100.0).to(device) \n",
    "    \n",
    "    step = sims_matrix.size(0)//num_tasks + 1\n",
    "    start = rank*step\n",
    "    end = min(sims_matrix.size(0),start+step)    \n",
    "    \n",
    "    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)): \n",
    "        \n",
    "        topk_sim, topk_idx = sims.topk(k=config['k_test'], dim=0)\n",
    "        topk_idx = topk_idx.cpu()  # Move indices to CPU\n",
    "        encoder_output = video_feats[topk_idx].to(device,non_blocking=True) \n",
    "        encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device,non_blocking=True) \n",
    "        output = model.text_encoder(text_ids[start+i].repeat(config['k_test'],1), \n",
    "                                    attention_mask = text_atts[start+i].repeat(config['k_test'],1),\n",
    "                                    encoder_hidden_states = encoder_output,\n",
    "                                    encoder_attention_mask = encoder_att,                             \n",
    "                                    return_dict = True,\n",
    "                                   )\n",
    "        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\n",
    "        score_matrix_t2v[start+i,topk_idx] = score + topk_sim\n",
    "\n",
    "    if args.distributed:\n",
    "        dist.barrier()   \n",
    "        torch.distributed.all_reduce(score_matrix_v2t, op=torch.distributed.ReduceOp.SUM) \n",
    "        torch.distributed.all_reduce(score_matrix_t2v, op=torch.distributed.ReduceOp.SUM)        \n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Evaluation time {}'.format(total_time_str)) \n",
    "\n",
    "    return score_matrix_v2t.cpu().numpy(), score_matrix_t2v.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Metric calculation function            \n",
    "@torch.no_grad()\n",
    "def itm_eval(scores_v2t, scores_t2v, txt2vmg, vid2txt):\n",
    "    \n",
    "    #Video->Text \n",
    "    ranks = np.zeros(scores_v2t.shape[0])\n",
    "    for index,score in enumerate(scores_v2t):\n",
    "        inds = np.argsort(score)[::-1]\n",
    "        ranks[index] = np.where(inds == vid2txt[index])[0][0]\n",
    "\n",
    "    # Compute metrics\n",
    "    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
    "  \n",
    "    #Text->Video \n",
    "    ranks = np.zeros(scores_t2v.shape[0])\n",
    "    \n",
    "    for index,score in enumerate(scores_t2v):\n",
    "        inds = np.argsort(score)[::-1]\n",
    "        ranks[index] = np.where(inds == txt2vmg[index])[0][0]\n",
    "    \n",
    "    mdR = np.median(ranks+1)\n",
    "        \n",
    "    # Compute metrics\n",
    "    vr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "    vr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "    vr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)        \n",
    "\n",
    "    tr_mean = (tr1 + tr5 + tr10) / 3\n",
    "    vr_mean = (vr1 + vr5 + vr10) / 3\n",
    "    r_mean = (tr_mean + vr_mean) / 2\n",
    "\n",
    "    eval_result =  {'txt_r1': tr1,\n",
    "                    'txt_r5': tr5,\n",
    "                    'txt_r10': tr10,\n",
    "                    'txt_r_mean': tr_mean,\n",
    "                    'vid_r1': vr1,\n",
    "                    'vid_r5': vr5,\n",
    "                    'vid_r10': vr10,\n",
    "                    'vid_r_mean': vr_mean,\n",
    "                    'vid_mdR': mdR,\n",
    "                    'r_mean': r_mean}\n",
    "    return eval_result\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "def main(args, config):\n",
    "    utils.init_distributed_mode(args)    \n",
    "    \n",
    "    device = torch.device(args.device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    #### Dataset #### \n",
    "    print(\"Creating retrieval dataset\")\n",
    "    test_dataset = VideoDataset(config['video_root'],config['ann_root'],num_frm=config['num_frm_test'],\n",
    "                                max_img_size=config['image_size'], frm_sampling_strategy='uniform') \n",
    "    \n",
    "    # Add these diagnostic prints\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Video directory: {config['video_root']}\")\n",
    "    print(f\"Number of videos found: {len(test_dataset)}\")\n",
    "    print(f\"Number of videos in annotation: {len(test_dataset.video2txt)}\")\n",
    "    \n",
    "    # List a few video paths to verify\n",
    "    video_files = os.listdir(config['video_root'])\n",
    "    mp4_files = [f for f in video_files if f.endswith('.mp4')]\n",
    "    print(f\"\\nTotal MP4 files in directory: {len(mp4_files)}\")\n",
    "    print(\"First few videos in directory:\", mp4_files[:5])\n",
    "\n",
    "    print(f\"Batch size: {config['batch_size']}\")\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    #### Model #### \n",
    "    print(\"Creating model\")\n",
    "    model = blip_tsf_retrieval(pretrained=config['pretrained'], eval=True, image_size=config['image_size'], vit=config['vit'])\n",
    "    \n",
    "    model = model.to(device)   \n",
    "    \n",
    "    model_without_ddp = model\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        model_without_ddp = model.module   \n",
    "    \n",
    "    score_v2t, score_t2v, = evaluation(model_without_ddp, test_loader, model_without_ddp.tokenizer, device, config)\n",
    "\n",
    "    if utils.is_main_process():  \n",
    "\n",
    "        test_result = itm_eval(score_v2t, score_t2v, test_loader.dataset.txt2video, test_loader.dataset.video2txt)  \n",
    "        print(test_result)\n",
    "\n",
    "        log_stats = {**{f'{k}': v for k, v in test_result.items()},}\n",
    "        with open(os.path.join(args.output_dir, \"test_result.txt\"),\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")     \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration cell - replace argparse\n",
    "config_path = './configs/retrieval_msrvtt_test.yaml'\n",
    "yaml = ruamel.yaml.YAML(typ='safe')  # Create a YAML object with safe loading\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "# Define args as a simple namespace object\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.config = config_path\n",
    "        self.output_dir = 'output/Retrieval_msrvtt'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.seed = 42\n",
    "        self.world_size = 1\n",
    "        self.dist_url = 'env://'\n",
    "        self.distributed = False  # Changed to False for notebook usage\n",
    "        self.gpu = 0\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Create output directory\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config updated:\n",
      "- Changed ann_root from 'annotation/data/refined_msrvtt_ret' to 'annotation/data/refined_msrvtt_ret'\n",
      "\n",
      "Verification:\n",
      "- Annotation directory exists: True\n",
      "- Video directory exists: True\n",
      "- Found annotation files: ['val.jsonl', 'test.jsonl', 'train_with_duplicates.jsonl', 'train.jsonl']\n"
     ]
    }
   ],
   "source": [
    "def update_config():\n",
    "    # Load the current config\n",
    "    yaml = ruamel.yaml.YAML()\n",
    "    config_path = './configs/retrieval_msrvtt_test.yaml'\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.load(f)\n",
    "    \n",
    "    # Update the annotation path\n",
    "    old_path = config['ann_root']\n",
    "    config['ann_root'] = 'annotation/data/refined_msrvtt_ret'\n",
    "    \n",
    "    # Save the updated config\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    print(\"Config updated:\")\n",
    "    print(f\"- Changed ann_root from '{old_path}' to '{config['ann_root']}'\")\n",
    "    print(\"\\nVerification:\")\n",
    "    print(f\"- Annotation directory exists: {os.path.exists(config['ann_root'])}\")\n",
    "    print(f\"- Video directory exists: {os.path.exists(config['video_root'])}\")\n",
    "    print(f\"- Found annotation files: {os.listdir(os.path.join(config['ann_root'], 'txt'))}\")\n",
    "\n",
    "update_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch calculations for each split:\n",
      "Batch size: 64\n",
      "--------------------------------------------------\n",
      "\n",
      "train.jsonl:\n",
      "- Unique videos: 7010\n",
      "- Number of batches: 110\n",
      "- Total captions: 140200\n",
      "\n",
      "val.jsonl:\n",
      "- Unique videos: 1000\n",
      "- Number of batches: 16\n",
      "- Total captions: 1000\n",
      "\n",
      "test.jsonl:\n",
      "- Unique videos: 1000\n",
      "- Number of batches: 16\n",
      "- Total captions: 1000\n",
      "\n",
      "Total across all splits:\n",
      "- Total unique videos: 9010\n",
      "- Total batches if processing all splits: 141\n",
      "\n",
      "Estimated memory usage per batch:\n",
      "- Batch size: 64 videos\n",
      "- Approximate memory per batch: 3200MB\n"
     ]
    }
   ],
   "source": [
    "def calculate_batch_info():\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    batch_size = 64  # from your config\n",
    "    txt_dir = Path('./annotation/data/msrvtt_ret/txt')\n",
    "    \n",
    "    print(\"Batch calculations for each split:\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total_videos = 0\n",
    "    for split in ['train.jsonl', 'val.jsonl', 'test.jsonl']:\n",
    "        file_path = txt_dir / split\n",
    "        if file_path.exists():\n",
    "            with open(file_path) as f:\n",
    "                lines = f.readlines()\n",
    "                unique_videos = len(set(json.loads(line)['clip_name'] for line in lines))\n",
    "                num_batches = (unique_videos + batch_size - 1) // batch_size\n",
    "                total_videos += unique_videos\n",
    "                \n",
    "                print(f\"\\n{split}:\")\n",
    "                print(f\"- Unique videos: {unique_videos}\")\n",
    "                print(f\"- Number of batches: {num_batches}\")\n",
    "                print(f\"- Total captions: {len(lines)}\")\n",
    "    \n",
    "    print(f\"\\nTotal across all splits:\")\n",
    "    print(f\"- Total unique videos: {total_videos}\")\n",
    "    print(f\"- Total batches if processing all splits: {(total_videos + batch_size - 1) // batch_size}\")\n",
    "    \n",
    "    # Memory usage estimate (rough calculation)\n",
    "    avg_video_size_mb = 50  # rough estimate, adjust based on your actual videos\n",
    "    print(f\"\\nEstimated memory usage per batch:\")\n",
    "    print(f\"- Batch size: {batch_size} videos\")\n",
    "    print(f\"- Approximate memory per batch: {batch_size * avg_video_size_mb}MB\")\n",
    "\n",
    "calculate_batch_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Using device: cuda\n",
      "Creating retrieval dataset\n",
      "Using downloaded and verified file: annotation/data/refined_msrvtt_ret/msrvtt_test.jsonl\n",
      "\n",
      "Dataset Statistics:\n",
      "Video directory: ./video_data\n",
      "Number of videos found: 1000\n",
      "Number of videos in annotation: 1000\n",
      "\n",
      "Total MP4 files in directory: 10000\n",
      "First few videos in directory: ['video6664.mp4', 'video9704.mp4', 'video8582.mp4', 'video2885.mp4', 'video4403.mp4']\n",
      "Batch size: 8\n",
      "Creating model\n",
      "Trainable parameter: visual_encoder.model.cls_token\n",
      "Trainable parameter: visual_encoder.model.pos_embed\n",
      "Trainable parameter: visual_encoder.model.time_embed\n",
      "Trainable parameter: visual_encoder.model.patch_embed.proj.weight\n",
      "Trainable parameter: visual_encoder.model.patch_embed.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.0.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.0.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.0.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.0.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.0.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.0.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.0.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.0.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.0.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.0.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.0.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.0.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.0.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.0.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.0.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.0.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.0.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.0.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.0.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.0.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.1.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.1.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.1.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.1.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.1.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.1.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.1.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.1.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.1.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.1.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.1.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.1.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.1.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.1.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.1.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.1.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.1.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.1.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.1.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.1.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.2.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.2.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.2.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.2.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.2.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.2.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.2.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.2.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.2.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.2.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.2.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.2.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.2.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.2.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.2.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.2.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.2.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.2.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.2.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.2.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.3.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.3.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.3.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.3.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.3.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.3.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.3.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.3.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.3.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.3.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.3.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.3.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.3.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.3.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.3.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.3.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.3.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.3.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.3.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.3.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.4.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.4.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.4.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.4.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.4.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.4.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.4.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.4.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.4.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.4.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.4.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.4.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.4.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.4.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.4.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.4.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.4.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.4.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.4.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.4.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.5.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.5.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.5.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.5.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.5.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.5.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.5.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.5.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.5.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.5.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.5.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.5.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.5.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.5.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.5.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.5.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.5.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.5.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.5.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.5.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.6.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.6.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.6.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.6.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.6.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.6.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.6.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.6.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.6.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.6.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.6.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.6.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.6.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.6.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.6.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.6.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.6.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.6.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.6.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.6.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.7.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.7.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.7.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.7.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.7.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.7.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.7.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.7.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.7.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.7.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.7.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.7.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.7.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.7.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.7.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.7.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.7.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.7.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.7.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.7.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.8.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.8.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.8.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.8.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.8.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.8.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.8.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.8.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.8.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.8.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.8.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.8.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.8.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.8.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.8.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.8.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.8.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.8.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.8.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.8.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.9.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.9.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.9.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.9.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.9.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.9.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.9.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.9.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.9.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.9.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.9.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.9.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.9.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.9.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.9.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.9.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.9.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.9.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.9.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.9.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.10.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.10.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.10.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.10.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.10.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.10.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.10.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.10.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.10.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.10.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.10.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.10.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.10.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.10.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.10.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.10.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.10.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.10.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.10.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.10.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.11.norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.11.norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.11.attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.11.attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.11.attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.11.attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.11.temporal_norm1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.11.temporal_norm1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.11.temporal_attn.qkv.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.11.temporal_attn.qkv.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.11.temporal_attn.proj.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.11.temporal_attn.proj.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.11.temporal_fc.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.11.temporal_fc.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.11.norm2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.11.norm2.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.11.mlp.fc1.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.11.mlp.fc1.bias\n",
      "Trainable parameter: visual_encoder.model.blocks.11.mlp.fc2.weight\n",
      "Trainable parameter: visual_encoder.model.blocks.11.mlp.fc2.bias\n",
      "Trainable parameter: visual_encoder.model.norm.weight\n",
      "Trainable parameter: visual_encoder.model.norm.bias\n",
      "Trainable parameter: vision_proj.weight\n",
      "Trainable parameter: vision_proj.bias\n",
      "load checkpoint from ./output/Retrieval_msrvtt_train/checkpoint_best.pth\n",
      "missing keys:\n",
      "[]\n",
      "Starting evaluation with 1000 texts and 125 videos...\n",
      "Computing features for evaluation...\n",
      "Computing text features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text batches: 100%|██████████| 4/4 [00:00<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing video features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  34%|███▎      | 42/125 [01:01<02:01,  1.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 227\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args, config)\u001b[0m\n\u001b[1;32m    224\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39mDistributedDataParallel(model, device_ids\u001b[38;5;241m=\u001b[39m[args\u001b[38;5;241m.\u001b[39mgpu])\n\u001b[1;32m    225\u001b[0m     model_without_ddp \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodule   \n\u001b[0;32m--> 227\u001b[0m score_v2t, score_t2v, \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_without_ddp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_without_ddp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mis_main_process():  \n\u001b[1;32m    231\u001b[0m     test_result \u001b[38;5;241m=\u001b[39m itm_eval(score_v2t, score_t2v, test_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtxt2video, test_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mvideo2txt)  \n",
      "File \u001b[0;32m~/Repos/BLIP-TimeSformer/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(model, data_loader, tokenizer, device, config)\u001b[0m\n\u001b[1;32m     57\u001b[0m     video_embed \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(video_embed,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \n\u001b[1;32m     59\u001b[0m     video_feat \u001b[38;5;241m=\u001b[39m video_feat\u001b[38;5;241m.\u001b[39mview(B,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,video_feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 60\u001b[0m     video_feats\u001b[38;5;241m.\u001b[39mappend(\u001b[43mvideo_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     61\u001b[0m     video_embeds\u001b[38;5;241m.\u001b[39mappend(video_embed)\n\u001b[1;32m     63\u001b[0m video_feats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(video_feats,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    main(args, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
