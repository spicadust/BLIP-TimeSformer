{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in json files from annotation/data/coco/coco_small/\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "with open('annotation/data/coco/full/coco_karpathy_train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('annotation/data/coco/full/coco_karpathy_val.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open('annotation/data/coco/full/coco_karpathy_test.json', 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_images 164062\n",
      "train_images 113287\n",
      "val_images 5000\n",
      "test_images 5000\n"
     ]
    }
   ],
   "source": [
    "train_images = set([item['image'] for item in train_data])\n",
    "val_images = set([item['image'] for item in val_data])\n",
    "test_images = set([item['image'] for item in test_data])\n",
    "\n",
    "# also find all the images in the directories of ./coco/ in format e.g. train2014/COCO_train2014_000000401251.jpg\n",
    "all_images = []\n",
    "for split in ['train2014', 'val2014', 'test2014']:\n",
    "    split_dir = os.path.join('./coco/', split)\n",
    "    all_images.extend([os.path.join(split, f) for f in os.listdir(split_dir)])\n",
    "all_images = set(all_images)\n",
    "\n",
    "print('all_images', len(all_images))\n",
    "print('train_images', len(train_images))\n",
    "print('val_images', len(val_images))\n",
    "print('test_images', len(test_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# iterate through train_images and check if they are in all_images\n",
    "missing_train_images = train_images - all_images\n",
    "print(len(missing_train_images))\n",
    "\n",
    "missing_val_images = val_images - all_images\n",
    "print(len(missing_val_images))\n",
    "\n",
    "missing_test_images = test_images - all_images\n",
    "print(len(missing_test_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5667"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ratio = 0.01\n",
    "# sample the train, val, test images from train_data, val_data, test_data\n",
    "sample_train_data = random.sample(train_data, int(len(train_data) * sample_ratio))\n",
    "sample_val_data = random.sample(val_data, int(len(val_data) * sample_ratio))\n",
    "sample_test_data = random.sample(test_data, int(len(test_data) * sample_ratio))\n",
    "len(sample_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566747"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val2014/COCO_val2014_000000068352.jpg\n"
     ]
    }
   ],
   "source": [
    "# find the duplicates in sample_train_data that have the same image path\n",
    "image_data = {}\n",
    "for image in train_images:\n",
    "    image_data[image] = []\n",
    "for item in train_data:\n",
    "    image_data[item['image']].append(item)\n",
    "\n",
    "for image, data in image_data.items():\n",
    "    if len(data) > 1:\n",
    "        print(image)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new json file with the sample data, including only one of the duplicates\n",
    "cleaned_train_data = []\n",
    "for image, data in image_data.items():\n",
    "    cleaned_train_data.append(data[0])\n",
    "\n",
    "with open('annotation/data/coco/full/coco_karpathy_train_cleaned.json', 'w') as f:\n",
    "    json.dump(cleaned_train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113287"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('annotation/data/coco/full/coco_karpathy_train_cleaned.json', 'r') as f:\n",
    "    new_train_data = json.load(f)\n",
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original entries: 7010\n",
      "Unique entries: 7010\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def deduplicate_jsonl(input_file, output_file):\n",
    "    # Keep track of seen clip_names\n",
    "    seen_clips = set()\n",
    "    \n",
    "    # Store unique entries\n",
    "    unique_entries = []\n",
    "    \n",
    "    # Read input file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                clip_name = entry['clip_name']\n",
    "                \n",
    "                # Only keep first occurrence of each clip_name\n",
    "                if clip_name not in seen_clips:\n",
    "                    seen_clips.add(clip_name)\n",
    "                    unique_entries.append(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip lines that aren't valid JSON\n",
    "                continue\n",
    "    \n",
    "    # Write unique entries to output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(unique_entries)\n",
    "    \n",
    "    print(f\"Original entries: {len(seen_clips)}\")\n",
    "    print(f\"Unique entries: {len(unique_entries)}\")\n",
    "\n",
    "# Use the function\n",
    "input_file = \"annotation/data/train_msrvtt_ret/train.jsonl\"\n",
    "output_file = \"annotation/data/train_msrvtt_ret/train_deduped.jsonl\"\n",
    "deduplicate_jsonl(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
